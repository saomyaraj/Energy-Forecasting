{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvBnJdlBt_aZ",
        "outputId": "9dd86dd8-413a-43e8-cf53-53ae2499a874"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, Add, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I-UDGxdW2jSO"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the new dataset\n",
        "path = \"Dataset.csv\"\n",
        "data = pd.read_csv(path, parse_dates=['Datetime'])\n",
        "data.set_index('Datetime', inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "data.ffill(inplace=True)\n",
        "\n",
        "# Normalize features\n",
        "scalers = {}\n",
        "features = ['Temperature', 'Relative Humidity', 'Wind Speed', 'Precipitation', 'Is_Weekend_Holiday', 'AEP_MW']\n",
        "for feature in features:\n",
        "    scaler = MinMaxScaler()\n",
        "    data[feature] = scaler.fit_transform(data[[feature]])\n",
        "    scalers[feature] = scaler\n",
        "\n",
        "# Prepare sequences for training\n",
        "n_timesteps = 24  # Fixed size of previous 24 hours of context data\n",
        "n_future = 48  # Maximum number of future hours to predict\n",
        "\n",
        "X, Y = [], []\n",
        "for i in range(len(data) - n_timesteps - n_future):\n",
        "    X.append(data.iloc[i:i + n_timesteps][['Temperature', 'Relative Humidity', 'Wind Speed', 'Precipitation', 'Is_Weekend_Holiday']].values)\n",
        "    Y.append(data.iloc[i + n_timesteps:i + n_timesteps + n_future]['AEP_MW'].values.reshape(-1, 1))  # Predict up to 48 hours\n",
        "\n",
        "X, Y = np.array(X), np.array(Y)\n",
        "\n",
        "# Split into training and testing\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "Y_train, Y_test = Y[:split], Y[split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mJKwLc02YtS-"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-Head Self Attention\n",
        "    attention = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)\n",
        "    x = attention(inputs, inputs)\n",
        "    x = Dropout(dropout)(x)\n",
        "    res = Add()([x, inputs])\n",
        "    x = LayerNormalization(epsilon=1e-6)(res)\n",
        "\n",
        "    # Feed Forward Part\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dropout(dropout)(ff)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "def build_flexible_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(mlp_units, activation=\"relu\")(x)\n",
        "    x = Dropout(mlp_dropout)(x)\n",
        "\n",
        "    # Output layer for maximum prediction length (48 hours)\n",
        "    x = Dense(n_future)(x)\n",
        "    outputs = Reshape((n_future, 1))(x)  # Reshape for maximum output length\n",
        "\n",
        "    return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPAs3GN90v97",
        "outputId": "a46f29ad-d3d0-423e-c929-7dd48f3c7910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 18ms/step - loss: 0.0465 - mae: 0.1643 - val_loss: 0.0258 - val_mae: 0.1339 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 17ms/step - loss: 0.0258 - mae: 0.1291 - val_loss: 0.0256 - val_mae: 0.1335 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 17ms/step - loss: 0.0253 - mae: 0.1279 - val_loss: 0.0258 - val_mae: 0.1341 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 17ms/step - loss: 0.0249 - mae: 0.1268 - val_loss: 0.0254 - val_mae: 0.1331 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 12ms/step - loss: 0.0249 - mae: 0.1269 - val_loss: 0.0257 - val_mae: 0.1340 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 0.0248 - mae: 0.1266 - val_loss: 0.0259 - val_mae: 0.1348 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 13ms/step - loss: 0.0246 - mae: 0.1261 - val_loss: 0.0265 - val_mae: 0.1364 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - loss: 0.0246 - mae: 0.1260 - val_loss: 0.0253 - val_mae: 0.1326 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12ms/step - loss: 0.0246 - mae: 0.1260 - val_loss: 0.0254 - val_mae: 0.1333 - learning_rate: 5.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 0.0246 - mae: 0.1260 - val_loss: 0.0260 - val_mae: 0.1349 - learning_rate: 5.0000e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - loss: 0.0245 - mae: 0.1258 - val_loss: 0.0258 - val_mae: 0.1345 - learning_rate: 5.0000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 14ms/step - loss: 0.0245 - mae: 0.1257 - val_loss: 0.0258 - val_mae: 0.1346 - learning_rate: 2.5000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m3030/3030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 14ms/step - loss: 0.0245 - mae: 0.1257 - val_loss: 0.0261 - val_mae: 0.1355 - learning_rate: 2.5000e-05\n"
          ]
        }
      ],
      "source": [
        "# Build and compile the model once\n",
        "input_shape = (n_timesteps, X_train.shape[2])  # Input shape is fixed to 24 hours of historical data\n",
        "\n",
        "model = build_flexible_transformer_model(\n",
        "    input_shape=input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=512,\n",
        "    num_transformer_blocks=6,\n",
        "    mlp_units=256,\n",
        "    dropout=0.1,\n",
        "    mlp_dropout=0.1\n",
        ")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_test, Y_test),\n",
        "    epochs=20,  # Increase epochs as needed\n",
        "    batch_size=32,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('flexible_electricity_forecast_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1inIY7611A5b"
      },
      "outputs": [],
      "source": [
        "def prepare_input_for_prediction(historical_data):\n",
        "    \"\"\"\n",
        "    Prepares the input data for prediction, using only historical data.\n",
        "\n",
        "    Args:\n",
        "    - historical_data (pd.DataFrame): DataFrame with the historical data.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: Prepared input data for the model.\n",
        "    \"\"\"\n",
        "    historical_features = ['Temperature', 'Relative Humidity', 'Wind Speed', 'Precipitation', 'Is_Weekend_Holiday']\n",
        "    # Ensure historical data has the correct features and is normalized\n",
        "    historical_input = historical_data[historical_features].values\n",
        "    # Normalize historical input data\n",
        "    for feature in historical_features:\n",
        "        historical_input[:, historical_features.index(feature)] = scalers[feature].transform(historical_input[:, historical_features.index(feature)].reshape(-1, 1)).reshape(-1)\n",
        "    return historical_input.reshape(1, 24, len(historical_features))  # Reshape for model input\n",
        "\n",
        "def predict_next_n_hours(model, historical_data, forecast_data, n_hours):\n",
        "    # Prepare historical input\n",
        "    historical_input = prepare_input_for_prediction(historical_data)\n",
        "\n",
        "    # Prepare forecast input\n",
        "    forecast_input = forecast_data.values.reshape(1, n_hours, 5)\n",
        "\n",
        "    # Combine the most recent historical data with forecast data\n",
        "    combined_input = np.concatenate([historical_input[:, -24+n_hours:, :], forecast_input], axis=1)\n",
        "\n",
        "    # Ensure we have exactly 24 hours of input data\n",
        "    if combined_input.shape[1] > 24:\n",
        "        combined_input = combined_input[:, -24:, :]\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(combined_input)\n",
        "    return scalers['AEP_MW'].inverse_transform(predictions[0, :n_hours].reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wggzH6ma_TIl"
      },
      "outputs": [],
      "source": [
        "def get_manual_input(hours, is_forecast=False):\n",
        "    manual_data = []\n",
        "    data_type = \"forecast\" if is_forecast else \"historical\"\n",
        "    print(f\"Please enter the {data_type} data for the {'next' if is_forecast else 'previous'} {hours} hours.\")\n",
        "    for i in range(hours):\n",
        "        print(f\"Hour {i+1}:\")\n",
        "        temperature = float(input(\"  Temperature: \"))\n",
        "        humidity = float(input(\"  Relative Humidity: \"))\n",
        "        wind_speed = float(input(\"  Wind Speed: \"))\n",
        "        precipitation = float(input(\"  Precipitation: \"))\n",
        "        is_weekend_holiday = int(input(\"  Is Weekend/Holiday (1 for Yes, 0 for No): \"))\n",
        "        manual_data.append([temperature, humidity, wind_speed, precipitation, is_weekend_holiday])\n",
        "\n",
        "    columns = ['Temperature', 'Relative Humidity', 'Wind Speed', 'Precipitation', 'Is_Weekend_Holiday']\n",
        "    df_manual = pd.DataFrame(manual_data, columns=columns)\n",
        "\n",
        "    for feature in columns:\n",
        "        df_manual[feature] = scalers[feature].transform(df_manual[[feature]])\n",
        "\n",
        "    return df_manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiiPC3fjkyVB",
        "outputId": "2c2fa8e8-56c3-494e-8d39-53822de7cf5f"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('flexible_electricity_forecast_model.keras')\n",
        "\n",
        "# Ask user for the prediction horizon\n",
        "n_hours = int(input(\"Enter the number of hours for prediction (3, 6, 12, 24, 48): \"))\n",
        "date = input(\"Enter the date for prediction (YYYY-MM-DD HH:MM:SS): \")\n",
        "\n",
        "# Determine if historical data is available\n",
        "historical_end = pd.to_datetime(date) - pd.Timedelta(hours=1)\n",
        "historical_start = historical_end - pd.Timedelta(hours=23)\n",
        "\n",
        "if historical_start not in data.index or historical_end not in data.index:\n",
        "    print(\"Historical data not available in dataset. Please enter the previous 24 hours of data manually.\")\n",
        "    historical_data = get_manual_input(24)\n",
        "else:\n",
        "    historical_data = data.loc[historical_start:historical_end]\n",
        "    print(\"Using historical data from the dataset.\")\n",
        "\n",
        "# Ensure we have 24 hours of historical data\n",
        "if len(historical_data) < 24:\n",
        "    print(\"Warning: Not enough historical data. Please provide the missing data manually.\")\n",
        "    missing_hours = 24 - len(historical_data)\n",
        "    manual_historical = get_manual_input(missing_hours)\n",
        "    historical_data = pd.concat([manual_historical, historical_data])\n",
        "\n",
        "# Get forecast data for the next n_hours\n",
        "print(f\"\\nNow, please enter the weather forecast data for the next {n_hours} hours:\")\n",
        "forecast_data = get_manual_input(n_hours, is_forecast=True)\n",
        "\n",
        "# Predict the next N hours\n",
        "predicted_values = predict_next_n_hours(model, historical_data, forecast_data, n_hours)\n",
        "\n",
        "# Print the results\n",
        "for i, value in enumerate(predicted_values):\n",
        "    print(f\"Hour {i+1}: {value[0]:.2f} MW\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
